# Memory Reduction:
Quantization
Mixed-precision training (as much float16 as possible)
Distributed Data Parallel (this one maybe not as much, but good for speed)
LoRA

CUDA memory logging
Matplotlib loss plot
