
[notice] A new release of pip is available: 24.0 -> 24.3.1
[notice] To update, run: pip install --upgrade pip

[notice] A new release of pip is available: 24.0 -> 24.3.1
[notice] To update, run: pip install --upgrade pip
Training:   0%|          | 5/2051 [04:40<26:32:05, 46.69s/it, loss=8.18]Training:   0%|          | 6/2051 [04:40<26:32:25, 46.72s/it, loss=8.18]Training:   0%|          | 5/2051 [04:39<26:25:50, 46.51s/it, loss=8.02]Training:   0%|          | 6/2051 [04:39<26:28:20, 46.60s/it, loss=8.02]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Training:   0%|          | 6/2051 [05:27<26:32:25, 46.72s/it, loss=8.54]Training:   0%|          | 7/2051 [05:27<26:33:16, 46.77s/it, loss=8.54]Training:   0%|          | 6/2051 [05:26<26:28:20, 46.60s/it, loss=8.27]Training:   0%|          | 7/2051 [05:26<26:30:26, 46.69s/it, loss=8.27]Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.04s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:13<00:13, 13.35s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  8.85s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:18<00:00,  9.48s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  9.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:19<00:00,  9.80s/it]
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
Training:   0%|          | 0/2051 [00:00<?, ?it/s]Training:   0%|          | 0/2051 [00:00<?, ?it/s]Training:   0%|          | 7/2051 [06:14<26:33:16, 46.77s/it, loss=6.61]Training:   0%|          | 8/2051 [06:14<26:34:59, 46.84s/it, loss=6.61]Training:   0%|          | 7/2051 [06:13<26:30:26, 46.69s/it, loss=6.53]Training:   0%|          | 8/2051 [06:13<26:33:08, 46.79s/it, loss=6.53]Training:   0%|          | 0/2051 [00:48<?, ?it/s, loss=7.5]Training:   0%|          | 1/2051 [00:48<27:40:49, 48.61s/it, loss=7.5]Training:   0%|          | 0/2051 [00:48<?, ?it/s, loss=7.55]Training:   0%|          | 1/2051 [00:48<27:32:42, 48.37s/it, loss=7.55]Training:   0%|          | 8/2051 [07:01<26:34:59, 46.84s/it, loss=5.85]Training:   0%|          | 9/2051 [07:01<26:37:03, 46.93s/it, loss=5.85]Training:   0%|          | 8/2051 [07:00<26:33:08, 46.79s/it, loss=6.43]Training:   0%|          | 9/2051 [07:00<26:35:58, 46.89s/it, loss=6.43]Training:   0%|          | 1/2051 [01:35<27:40:49, 48.61s/it, loss=11.4]Training:   0%|          | 2/2051 [01:35<26:57:41, 47.37s/it, loss=11.4]Training:   0%|          | 1/2051 [01:34<27:32:42, 48.37s/it, loss=7.67]Training:   0%|          | 2/2051 [01:34<26:45:00, 47.00s/it, loss=7.67]