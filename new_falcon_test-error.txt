.env: line 1: HUGGING_FACE_HUB_TOKEN: command not found
/scratch/mjd9571/conda/envs_dirs/juypter/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:809: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.
  warnings.warn(
Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]Loading checkpoint shards:  11%|█         | 1/9 [00:07<01:03,  7.88s/it]Loading checkpoint shards:  22%|██▏       | 2/9 [00:12<00:41,  5.96s/it]Loading checkpoint shards:  33%|███▎      | 3/9 [00:17<00:32,  5.40s/it]Loading checkpoint shards:  44%|████▍     | 4/9 [00:22<00:25,  5.15s/it]Loading checkpoint shards:  56%|█████▌    | 5/9 [00:27<00:20,  5.12s/it]Loading checkpoint shards:  67%|██████▋   | 6/9 [00:31<00:15,  5.01s/it]Loading checkpoint shards:  78%|███████▊  | 7/9 [00:36<00:09,  4.94s/it]Loading checkpoint shards:  89%|████████▉ | 8/9 [00:41<00:04,  4.84s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:44<00:00,  4.47s/it]Loading checkpoint shards: 100%|██████████| 9/9 [00:44<00:00,  4.99s/it]
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
Setting `pad_token_id` to `eos_token_id`:None for open-end generation.
